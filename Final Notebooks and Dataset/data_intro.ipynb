{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WF3Ht6CzRAaB",
    "toc": true
   },
   "source": [
    "# CE9010 Group Project: Github Issue Title Generation\n",
    "1. [Introduction](#introduction)\n",
    "2. [Step1: Identify data problem ](#step1)\n",
    "    * [Sub paragraph](#subparagraph1)\n",
    "3. [Step2: Data Acquisition](#step2)\n",
    "    * [Sub paragraph](#subparagraph1)\n",
    "4. [Step3: Data Exploration](#step3)\n",
    "5. [Step4: Data Pre-Processing](#step4)\n",
    "6. [Step5: Data Analysis](#step5)\n",
    "    * [Load issue body and issue title](#load_data)\n",
    "    * [Sequence to sequence model for text summarization](#model)\n",
    "      + [Encoder](#encoder)\n",
    "      + [Decoder](#decoder)\n",
    "      + [Model compilation](#compile)\n",
    "      + [Model architecture summary](#summary)\n",
    "      + [Model training](#train)\n",
    "     * [A few examples](#examples)\n",
    "     * [ROUGE scores](#ROUGE)\n",
    "6. [Step6: Hyper-Parameter Tuning](#step6)\n",
    "6. [Step7: Result Analysis](#step7)\n",
    "6. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q0HqWw57d2Mv"
   },
   "outputs": [],
   "source": [
    "# Note: \n",
    "# Install the libraries: \n",
    "# Machine Learning & Deep Learning: \n",
    "!pip install sklearn, pandas, keras, tensorflow\n",
    "\n",
    "# Data pre-processing. Note: ktext can only build with Python 3.6!\n",
    "!pip install ktext\n",
    "# Model Visualization\n",
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!pip install graphviz\n",
    "!brew install graphviz\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xENg5ULpYfz9"
   },
   "source": [
    "## Step1: Identify Data Problem<a name=\"step1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCiSwevSRAaC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-VpHHP_z9sO"
   },
   "source": [
    "There are **2.5 quintillion** bytes of data created each day at our current pace, and one of the most valuable parts is the natural language part. Thousands of millions of text information is generated every second, including news articles, analysis reports, even your review on Yelp about a movie, or your review on IMDB regarding a movie. \n",
    "\n",
    "However, extracting useful information from these natural human languages has always been difficult since they can hardly be transformed into digital data or at least, they cannot be transformed in an efficient and informative way. \n",
    "\n",
    "Thankfully, Long Short-Term Memory algorithm which is a branch of natural language processing can mimic human recognition processes and effectively address the problem. \n",
    "\n",
    "In this project, we aim to use machines to summarise a piece of article. To put it simply, we would like to devise an algorithm that can generate a title for an essay, no matter in what language it is written,  no matter how long it is and no matter what genre it is.\n",
    "\n",
    "This application is profound and able to shape a better future for human beings. For example, it can make our communication efficient. People do not need to spend hours on thinking a proper title for their emails. Editors can also use the titles done by machines for references. More importantly, the researchers in academia could use this to read papers faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Maun_qYcRAaF"
   },
   "source": [
    "## Step2: Data Acquisition<a name=\"step2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9uZHy-OeRAaG"
   },
   "source": [
    "Look at filesystem to see files extracted from BigQuery (or Kaggle: https://www.kaggle.com/davidshinn/github-issues/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tcupk2sRSil"
   },
   "outputs": [],
   "source": [
    "!ls -lah | grep github_issues.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Afss-qfTRAaJ"
   },
   "source": [
    "Split data into train and test set and preview data"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JTWAo-B8RAad"
   ],
   "name": "summarizer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
