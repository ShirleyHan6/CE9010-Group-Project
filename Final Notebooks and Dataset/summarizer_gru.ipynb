{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tcupk2sRSil"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "with K.tf.device('/gpu:1'):\n",
    "    config = tf.ConfigProto(device_count = {'CPU' : 1, 'GPU' : 1})\n",
    "    session = tf.Session(config=config)\n",
    "    K.set_session(session)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Afss-qfTRAaJ"
   },
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diiH4hr_Z-7F"
   },
   "source": [
    "## Step3: Data Exploration<a name=\"step3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CFMEs2a_aRuW"
   },
   "source": [
    "## Step 4: Data Pre-Processing<a name=\"step4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICV9kd9KaPzC"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pTOdxEZRAaK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 900,000 rows 3 columns\n",
      "Test: 100,000 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4017961</th>\n",
       "      <td>\"https://github.com/MoOx/postcss-cssnext/issues/423\"</td>\n",
       "      <td>replace postcss-nesting to postcss-nested</td>\n",
       "      <td>replace postcss-nesting with postcss-nested https://github.com/postcss/postcss-nested because of the second one is more powerful.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1217222</th>\n",
       "      <td>\"https://github.com/cal-wilkes/waveform_jenny-/issues/15\"</td>\n",
       "      <td>create triangle wave generator</td>\n",
       "      <td>this module will take in input from the signals freq produced by the frequency counter module and amp produced by the amplitude counter module. this module will also be supplied by the clock signal to allow for synchronous operation and a reset signal to allow for a system reset.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477951</th>\n",
       "      <td>\"https://github.com/Sym123Blue/GitHubQAAutomation37dce3ec-9a11-4271-b461-dc301665815b/issues/1\"</td>\n",
       "      <td>github automation for issue title 37dce3ec-9a11-4271-b461-dc301665815b</td>\n",
       "      <td>github automation for issue body 37dce3ec-9a11-4271-b461-dc301665815b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                               issue_url  \\\n",
       "4017961                                             \"https://github.com/MoOx/postcss-cssnext/issues/423\"   \n",
       "1217222                                        \"https://github.com/cal-wilkes/waveform_jenny-/issues/15\"   \n",
       "477951   \"https://github.com/Sym123Blue/GitHubQAAutomation37dce3ec-9a11-4271-b461-dc301665815b/issues/1\"   \n",
       "\n",
       "                                                                    issue_title  \\\n",
       "4017961                               replace postcss-nesting to postcss-nested   \n",
       "1217222                                          create triangle wave generator   \n",
       "477951   github automation for issue title 37dce3ec-9a11-4271-b461-dc301665815b   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                             body  \n",
       "4017961                                                                                                                                                         replace postcss-nesting with postcss-nested https://github.com/postcss/postcss-nested because of the second one is more powerful.  \n",
       "1217222  this module will take in input from the signals freq produced by the frequency counter module and amp produced by the amplitude counter module. this module will also be supplied by the clock signal to allow for synchronous operation and a reset signal to allow for a system reset.  \n",
       "477951                                                                                                                                                                                                                      github automation for issue body 37dce3ec-9a11-4271-b461-dc301665815b  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data sample 2M rows\n",
    "traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=1000000), \n",
    "                                   test_size=.10)\n",
    "\n",
    "\n",
    "# Data statistics\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# Data preview\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jgMFuhBUTiv"
   },
   "source": [
    "Keep English Language Title only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a-MJQbSdUS4o"
   },
   "outputs": [],
   "source": [
    "#traindf\n",
    "eng_rec = list()\n",
    "for i in range(len(traindf)):\n",
    "    temp_ord = ord(traindf.iloc[i,:].issue_title[0])\n",
    "    if((temp_ord >= ord('a') and temp_ord <= ord('z'))\n",
    "       or(temp_ord >= ord('A') and temp_ord <= ord('Z'))or(temp_ord >= ord('0') and temp_ord <= ord('9'))): \n",
    "        eng_rec.append(i)\n",
    "traindf = traindf.iloc[eng_rec]\n",
    "#testdf\n",
    "eng_rec = list()\n",
    "for i in range(len(testdf)):\n",
    "    temp_ord = ord(testdf.iloc[i,:].issue_title[0])\n",
    "    if((temp_ord >= ord('a') and temp_ord <= ord('z'))\n",
    "       or(temp_ord >= ord('A') and temp_ord <= ord('Z'))or(temp_ord >= ord('0') and temp_ord <= ord('9'))): \n",
    "        eng_rec.append(i)\n",
    "testdf = testdf.iloc[eng_rec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5ZWzEMHERAaM"
   },
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PpXrAnTJRAaM"
   },
   "outputs": [],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqYeQ2EuRAaT"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCGPjWtFRAaW"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 294 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 52 sec\n",
      "WARNING:root:Finished parsing 882,034 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 43 sec\n"
     ]
    }
   ],
   "source": [
    "# Clean, tokenize, and apply padding / truncating to make the length of each document the same(70)\n",
    "# Only use the top 8,000 words in the vocabulary and set the remaining words to 1 which will stand for the index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TymQOQgvRAaY"
   },
   "source": [
    "#### Processed issue body example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-4gxUzPsRAaZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " replace postcss-nesting with postcss-nested https://github.com/postcss/postcss-nested because of the second one is more powerful. \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  777\n",
      " 4187 6408   18 4187 1898   13  158   11    3  555   68    8   97 5464] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjjWpnjXRAab"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 52 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 8 sec\n",
      "WARNING:root:Finished parsing 882,034 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 11 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JTWAo-B8RAad"
   },
   "source": [
    "#### Processed issue title example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qddut_tdRAae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " replace postcss-nesting to postcss-nested\n",
      "after pre-processing:\n",
      " [   2  403 4118 4020    4 4118  758    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S__BJY9xRAaf"
   },
   "source": [
    "Dump the body and title data for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FnFSRt9pRAag"
   },
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp1.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp1.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs1.npy', train_title_vecs)\n",
    "np.save('train_body_vecs1.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mgynsl0iRAah"
   },
   "source": [
    "## Step5: Data Analysis<a name=\"step5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ez5v5ytKRAai"
   },
   "source": [
    "### Load the data from disk into variables<a name=\"load_data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AUeNaiD-RAaj"
   },
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hoVTwML5RAam"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (882034, 70)\n",
      "Shape of decoder input: (882034, 11)\n",
      "Shape of decoder target: (882034, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs1.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eTbNSZ_8RAao"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp1.dpkl: 8,002\n",
      "Size of vocabulary for title_pp1.dpkl: 4,502\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp1.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('title_pp1.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fE_JIcktRAar"
   },
   "source": [
    "### Define Model Architecture<a name=\"model\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jiF1khwmRAas"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, GRU, Dense, Embedding, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GElcd1a-sh8s"
   },
   "source": [
    "#### Encoder<a name=\"encoder\"></a>\n",
    "The encoder consists of a word embedding layer for the issue body vectors, a batch normalization layer for the embedding layer and a Gated Recurrent Unit layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7jiayVchsfgy"
   },
   "outputs": [],
   "source": [
    "# embedding and hidden units dimension\n",
    "latent_dim = 300\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding Layer\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "# Batch Normalization Layer\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Get the hidden state of the GRU\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "# Get output of the encoder\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aJGLxJmrso6E"
   },
   "source": [
    "#### Decoder <a name=\"decoder\"></a>\n",
    "The decoder consists of an embedding layer for the issue title vectors, a batch normalization layer for the embedding layer, a Gated Recurrent Unit(GRU) layer, a batch normalization layer for the GRU layer and finally, a densely connected layer for the final output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KT5eeIqzRAav"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Decoder Word Embedding Layer\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "# Batch Normalization Layer\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "clfopKx3tPYo"
   },
   "source": [
    "#### Model Compilation<a name=\"compile\"></a>\n",
    "The initial \n",
    "Nadam optimizer was chosen because...\n",
    "We choose to use the sparse categorical crossentropy because..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j_sLb4JOtOki"
   },
   "outputs": [],
   "source": [
    "#### Seq2Seq Model ####\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K9iJTnAERAaz"
   },
   "source": [
    "#### Examine Model Architecture Summary <a name=\"summary\"></a>\n",
    "From the model visualization we can better understand the architecture of this model: <br>\n",
    "The model generates a single word forecast and calls it recursively. <br>\n",
    "\n",
    "That is, the decoder uses the context vector and the distributed representation of all words generated so far as input in order to generate the next word.<br>\n",
    "\n",
    "A language model can be used to interpret the sequence of words generated so far to provide a second context vector to combine with the representation of the source document in order to generate the next word in the sequence.<br>\n",
    "\n",
    "The summary is built by recursively calling the model with the previously generated word appended.<br>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2hRExBEhRAaz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1350600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4502)   1355102     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,191,702\n",
      "Trainable params: 6,189,902\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"410pt\" viewBox=\"0.00 0.00 541.50 410.00\" width=\"542pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 406)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-406 537.5,-406 537.5,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 140393703323800 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>140393703323800</title>\n",
       "<polygon fill=\"none\" points=\"59,-365.5 59,-401.5 265,-401.5 265,-365.5 59,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-378.9\">Decoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140393703323184 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>140393703323184</title>\n",
       "<polygon fill=\"none\" points=\"16,-292.5 16,-328.5 308,-328.5 308,-292.5 16,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-305.9\">Decoder-Word-Embedding: Embedding</text>\n",
       "</g>\n",
       "<!-- 140393703323800&#45;&gt;140393703323184 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>140393703323800-&gt;140393703323184</title>\n",
       "<path d=\"M162,-365.4551C162,-357.3828 162,-347.6764 162,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"165.5001,-338.5903 162,-328.5904 158.5001,-338.5904 165.5001,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393703316728 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>140393703316728</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 324,-255.5 324,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"162\" y=\"-232.9\">Decoder-Batchnorm-1: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140393703323184&#45;&gt;140393703316728 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>140393703323184-&gt;140393703316728</title>\n",
       "<path d=\"M162,-292.4551C162,-284.3828 162,-274.6764 162,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"165.5001,-265.5903 162,-255.5904 158.5001,-265.5904 165.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393472378528 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>140393472378528</title>\n",
       "<polygon fill=\"none\" points=\"328.5,-292.5 328.5,-328.5 533.5,-328.5 533.5,-292.5 328.5,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"431\" y=\"-305.9\">Encoder-Input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140394041282912 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>140394041282912</title>\n",
       "<polygon fill=\"none\" points=\"342,-219.5 342,-255.5 520,-255.5 520,-219.5 342,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"431\" y=\"-232.9\">Encoder-Model: Model</text>\n",
       "</g>\n",
       "<!-- 140393472378528&#45;&gt;140394041282912 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>140393472378528-&gt;140394041282912</title>\n",
       "<path d=\"M431,-292.4551C431,-284.3828 431,-274.6764 431,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"434.5001,-265.5903 431,-255.5904 427.5001,-265.5904 434.5001,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393703314992 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>140393703314992</title>\n",
       "<polygon fill=\"none\" points=\"218,-146.5 218,-182.5 374,-182.5 374,-146.5 218,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"296\" y=\"-159.9\">Decoder-GRU: GRU</text>\n",
       "</g>\n",
       "<!-- 140393703316728&#45;&gt;140393703314992 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>140393703316728-&gt;140393703314992</title>\n",
       "<path d=\"M195.1236,-219.4551C212.8402,-209.8035 234.8453,-197.8156 253.7779,-187.5016\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"255.686,-190.4479 262.793,-182.5904 252.3372,-184.3008 255.686,-190.4479\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140394041282912&#45;&gt;140393703314992 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>140394041282912-&gt;140393703314992</title>\n",
       "<path d=\"M397.6292,-219.4551C379.6993,-209.7596 357.4095,-197.7066 338.2773,-187.361\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"339.9159,-184.2682 329.4548,-182.5904 336.5863,-190.4257 339.9159,-184.2682\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393478535768 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>140393478535768</title>\n",
       "<polygon fill=\"none\" points=\"134,-73.5 134,-109.5 458,-109.5 458,-73.5 134,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"296\" y=\"-86.9\">Decoder-Batchnorm-2: BatchNormalization</text>\n",
       "</g>\n",
       "<!-- 140393703314992&#45;&gt;140393478535768 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>140393703314992-&gt;140393478535768</title>\n",
       "<path d=\"M296,-146.4551C296,-138.3828 296,-128.6764 296,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"299.5001,-119.5903 296,-109.5904 292.5001,-119.5904 299.5001,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 140393500967824 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>140393500967824</title>\n",
       "<polygon fill=\"none\" points=\"192,-.5 192,-36.5 400,-36.5 400,-.5 192,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"296\" y=\"-13.9\">Final-Output-Dense: Dense</text>\n",
       "</g>\n",
       "<!-- 140393478535768&#45;&gt;140393500967824 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>140393478535768-&gt;140393500967824</title>\n",
       "<path d=\"M296,-73.4551C296,-65.3828 296,-55.6764 296,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"299.5001,-46.5903 296,-36.5904 292.5001,-46.5904 299.5001,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3x2zfvXfRAa0"
   },
   "source": [
    "#### Train Model<a name=\"train\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LPaB9plORAa1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 776189 samples, validate on 105845 samples\n",
      "Epoch 1/5\n",
      "776189/776189 [==============================] - 107s 138us/step - loss: 2.5401 - val_loss: 2.4854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shirley/anaconda3/envs/py36/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model_2/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5\n",
      "776189/776189 [==============================] - 105s 136us/step - loss: 2.3526 - val_loss: 2.4395\n",
      "Epoch 3/5\n",
      "776189/776189 [==============================] - 102s 131us/step - loss: 2.2588 - val_loss: 2.4259\n",
      "Epoch 4/5\n",
      "776189/776189 [==============================] - 162s 209us/step - loss: 2.1947 - val_loss: 2.4238\n",
      "Epoch 5/5\n",
      "776189/776189 [==============================] - 112s 144us/step - loss: 2.1444 - val_loss: 2.4333\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 5\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HMxYVjo-RAa3"
   },
   "outputs": [],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VfsA0hlARAa5"
   },
   "source": [
    "#### See Results On Test Set<a name=\"examples\"></a>\n",
    "\n",
    "1. To predict the issue titles with our model, we wrote a script that extracts the encoder model  and decoder model, then predicts the title sequence word by word with the models. \n",
    "2. From the examples, we can see that the titles generated by our model are similar to the original titles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eKEh58kYRAa5"
   },
   "outputs": [],
   "source": [
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eyajdw5aRAa6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example # 44372\n",
      "Issue Body:\n",
      " compiled code doesn't seem to recognize keyword arguments. minimal example: hello.py def aaa thing=3 : print thing def bbb a : aaa aaa a aaa thing=a hello.html <script src= __javascript__/hello.js ></script> <script type= text/javascript > hello.bbb 1 ; </script> this produces output 3 1 3 , whereas in python bbb 1 obviously gives 3 1 1 . i compile this with transcrypt -b -m hello.py . i installed transcrypt with pip, my python version: python 3.6.3 |anaconda, inc.| default, oct 13 2017, 12:02:49 gcc 7.2.0 on linux why does it happen? \n",
      "\n",
      "Original Title: keyword arguments are ignored.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******: arguments to keyword arguments\n",
      "\n",
      "Example # 94207\n",
      "Issue Body:\n",
      " i cannot get to build the project with xcode 9.2 and swift4, i get a warning on autokbisw that tells me to conver to swift4 and the error below> ! schermata 2017-12-13 alle 13 45 24 https://user-images.githubusercontent.com/26550959/33939634-37415246-e00c-11e7-8749-0a097f1a506d.png \n",
      "\n",
      "Original Title: cannot build with swift4\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******: cannot build on xcode number\n",
      "\n",
      "Example # 93691\n",
      "Issue Body:\n",
      " as mentioned in 105 we have some slow queries, maybe we can't make them really fast, but at least we could improve the ui/ux i'd like to suggest the following loading spinner https://i.imgur.com/blfjfoc.gif \n",
      "\n",
      "Original Title: add loading spinner to slow queries.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******: improve ui loading spinner\n",
      "\n",
      "Example # 55923\n",
      "Issue Body:\n",
      " <!-- make the title of the issue: {game title id} - {game name} the title id for a game can be found in the log file generated by xenia when running the game through xenia. look for xex_header_execution_info . --> <!-- xbox 360 marketplace links only. can't find it? try google: game title site:marketplace.xbox.com --> marketplace https://marketplace.xbox.com/hu-hu/product/air-conflicts-vietnam/66acd000-77fe-1000-9115-d802413307d9 <!-- replace xenia commit id with a link to the commit the build you tested with is based from. in appveyor, you can find the link under the commit message. ie: tested on https://github.com/benvanik/xenia/commit/xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx --> tested on https://github.com/benvanik/xenia/commit/ee34b4923c3dc9ab895e78afaf2b925e8c39af43 issues: game starts shows the intros if i press start button and start the mission game crashes. > w> 00000028 error: fn export referenced getprocaddressbyordinal 0aff xampartygetuserlist is not implemented > !> 00000028 gpu: read from unknown register 044b > w> 00000004 gpu: write to unknown register 0082 = 00000000 > !> 00000028 resolvepath shaderdumpxe:\\comparebackends failed - no root found gpu writes to unk. register - 0081 - 0082 - 0e00 - 0e40 unimplemented - xampartygetuserlist - xampartysendgameinvites - xampartysetcustomdata - xampartygetbandwidth - xamshowpartyui - xamshowcommunitysessionsui log: acv.txt https://github.com/xenia-project/game-compatibility/files/1168365/acv.txt labels: - state-menus - kernel-unimplemented-features <!-- a list of current labels can be found here: https://github.com/xenia-project/game-compatibility/labels --> \n",
      "\n",
      "Original Title: 413307d9 - air conflicts: vietnam\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******: title of the game id\n",
      "\n",
      "Example # 11289\n",
      "Issue Body:\n",
      " hi ! i've a little problem here : i want to register a new store module in a plugin. in my plugin i do this : js import mystore from './store'; export default { install vue, { store } { store.registermodule 'custom', mystore ; } }; in my nuxt plugin i've this : js import customplugin from './custom/plugin'; export default async { store } => { vue.use customplugin, { store } ; }; then i call an action in my page : js export default { async fetch { store } { await store.dispatch 'custom/fetch' ; } } everything is ok on client-side but on server-side, i've this error: vuex unknown action type: custom/fetch i'm using nuxt-rc11 and my store files are located outside the root /store directory. it's possible to do that ? thanks ! \n",
      "\n",
      "Original Title: store.registermodule in plugin\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******: how to save custom action\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# choose 5 examples randomly from the test set\n",
    "seq2seq_inf.demo_model_predictions(n=5, issue_df=testdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uesE_-IbtZnX"
   },
   "source": [
    "#### ROUGE Scores <a name=\"rouge\"></a>\n",
    "ROUGE is a recall-oriented package for Automatic Evaluation of machine generated summaries. <br>\n",
    "The algorithms of ROUGE scores are presented as follows. <br>\n",
    "\n",
    "<hr>\n",
    "$$\n",
    "P_{l c s}=\\frac{L C S(X, Y)}{n}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R_{l c s}=\\frac{L C S(X, Y)}{m}\n",
    "$$\n",
    "\n",
    "$$\n",
    "F_{l c s}=\\frac{\\left(1+\\beta^{2}\\right) R_{l c s} P_{l c s}}{R_{l c s}+\\beta^{2} P_{l c s}}(4)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "60c0H7JJRAbF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "ROUGE-l-f: 0.0\n",
      "ROUGE-l-f: 181.297584877275\n",
      "ROUGE-l-f: 376.7957096457366\n",
      "ROUGE-l-f: 567.3355183080444\n",
      "ROUGE-l-f: 747.2491250976716\n",
      "ROUGE-l-f: 942.315243429852\n",
      "ROUGE-l-f: 1135.9542131118494\n",
      "ROUGE-l-f: 1333.1688210936354\n",
      "ROUGE-l-f: 1524.860930882192\n",
      "ROUGE-l-f: 1720.2050039483654\n",
      "ROUGE-l-f: 1909.3125374956046\n",
      "ROUGE-l-f: 2112.474692861815\n",
      "ROUGE-l-f: 2294.641018573923\n",
      "ROUGE-l-f: 2488.0898485973808\n",
      "ROUGE-l-f: 2675.2179891359565\n",
      "ROUGE-l-f: 2859.3999359251025\n",
      "ROUGE-l-f: 3058.3611048952116\n",
      "ROUGE-l-f: 3256.337239425404\n",
      "ROUGE-l-f: 3438.6828817665873\n",
      "ROUGE-l-f: 3625.4332700501\n",
      "ROUGE-l-f: 3817.459901720119\n",
      "ROUGE-l-f: 4009.854819024462\n",
      "ROUGE-l-f: 4195.462551967698\n",
      "ROUGE-l-f: 4383.035619050131\n",
      "ROUGE-l-f: 4587.322358995023\n",
      "ROUGE-l-f: 4774.048018157001\n",
      "ROUGE-l-f: 4970.359704550067\n",
      "ROUGE-l-f: 5158.457789596915\n",
      "ROUGE-l-f: 5343.014366980141\n",
      "ROUGE-l-f: 5528.790454453443\n",
      "ROUGE-l-f: 5719.151365922255\n",
      "ROUGE-l-f: 5907.170235509991\n",
      "ROUGE-l-f: 6096.907188775084\n",
      "ROUGE-l-f: 6272.184778668883\n",
      "ROUGE-l-f: 6443.472337004556\n",
      "ROUGE-l-f: 6628.250699568492\n",
      "ROUGE-l-f: 6811.706495654524\n",
      "ROUGE-l-f: 6997.859385659687\n",
      "ROUGE-l-f: 7188.650638566347\n",
      "ROUGE-l-f: 7370.224738396842\n",
      "ROUGE-l-f: 7559.111516099057\n",
      "ROUGE-l-f: 7746.383437580321\n",
      "ROUGE-l-f: 7932.548681552026\n",
      "ROUGE-l-f: 8122.6847433110825\n",
      "ROUGE-l-f: 8315.943519785771\n",
      "ROUGE-l-f: 8510.854943858441\n",
      "ROUGE-l-f: 8700.843955357648\n",
      "ROUGE-l-f: 8891.594196939786\n",
      "ROUGE-l-f: 9072.565725261735\n",
      "ROUGE-l-f: 9265.868386268237\n",
      "ROUGE-l-f: 9446.96671705064\n",
      "ROUGE-l-f: 9629.806469536352\n",
      "ROUGE-l-f: 9821.65271783207\n",
      "ROUGE-l-f: 10015.501010316853\n",
      "ROUGE-l-f: 10206.370128498489\n",
      "ROUGE-l-f: 10399.191052408709\n",
      "ROUGE-l-f: 10574.271237111268\n",
      "ROUGE-l-f: 10766.95571416039\n",
      "ROUGE-l-f: 10964.953177344569\n",
      "ROUGE-l-f: 11141.940807184248\n",
      "ROUGE-l-f: 11336.934979490296\n",
      "ROUGE-l-f: 11530.59280604923\n",
      "ROUGE-l-f: 11715.528190580626\n",
      "ROUGE-l-f: 11912.31956462653\n",
      "ROUGE-l-f: 12100.869799318192\n",
      "ROUGE-l-f: 12293.687980566327\n",
      "ROUGE-l-f: 12483.027776906863\n",
      "ROUGE-l-f: 12670.471084438903\n",
      "ROUGE-l-f: 12856.683775417505\n",
      "ROUGE-l-f: 13057.168115273387\n",
      "ROUGE-l-f: 13241.090207264113\n",
      "ROUGE-l-f: 13417.375047107282\n",
      "ROUGE-l-f: 13605.778606297754\n",
      "ROUGE-l-f: 13798.905400114156\n",
      "ROUGE-l-f: 13986.14439857137\n",
      "ROUGE-l-f: 14175.66356589314\n",
      "ROUGE-l-f: 14367.738418804945\n",
      "ROUGE-l-f: 14562.996565269139\n",
      "ROUGE-l-f: 14738.165922745646\n",
      "ROUGE-l-f: 14928.346534527582\n",
      "ROUGE-l-f: 15129.704115306593\n",
      "ROUGE-l-f: 15326.163833811093\n",
      "ROUGE-l-f: 15508.721926435706\n",
      "ROUGE-l-f: 15699.50163466241\n",
      "ROUGE-l-f: 15883.928978859512\n",
      "ROUGE-l-f: 16077.596527846543\n",
      "ROUGE-l-f: 16275.550064369898\n",
      "ROUGE-l-f: 16460.51931310403\n",
      "ROUGE-l-f: 16654.076313489342\n",
      "ROUGE-l-f: 16852.032151464617\n",
      "ROUGE-l-f: 17038.991089122595\n",
      "ROUGE-l-f: 17234.921604414838\n",
      "ROUGE-l-f: 17424.66182775874\n",
      "ROUGE-l-f: 17630.11878271953\n",
      "ROUGE-l-f: 17821.438795846585\n",
      "ROUGE-l-f: 18017.73834215901\n",
      "ROUGE-l-f: 18206.375113355207\n",
      "ROUGE-l-f: 18399.49783085637\n",
      "ROUGE-l-f: 18582.996063274575\n",
      "ROUGE-1: 0.21645244720210008\n",
      "ROUGE-2: 0.07449620598681908\n",
      "ROUGE-l: 0.18962468921126593\n",
      "Average of ROUGE-1, ROUGE-2 and ROUGE-l:  0.160191114133395\n"
     ]
    }
   ],
   "source": [
    "# Import the ROUGE Package\n",
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "test_title_text = testdf.issue_title.tolist()\n",
    "test_body_text = testdf.body.tolist()\n",
    "predict_title_text = [None]*len(test_body_text)\n",
    "rouge_1_f, rouge_2_f, rouge_l_f = 0, 0, 0\n",
    "\n",
    "length = len(test_body_text)\n",
    "\n",
    "# Generate the title for each issue body\n",
    "for i in range(length):\n",
    "    exm, predict_title_text[i] = seq2seq_inf.generate_issue_title(raw_input_text=test_body_text[i])\n",
    "    if i%1000==0:\n",
    "        print(i)\n",
    "        \n",
    "def calculate_rouge():\n",
    "    rouge_1_f, rouge_2_f, rouge_l_f = 0, 0, 0\n",
    "    for i in range(length):\n",
    "        # The rouge package does not accept empty strings \n",
    "        if predict_title_text[i]==\"\":\n",
    "            predict_title_text[i] = \"issue\"\n",
    "            \n",
    "        scores = rouge.get_scores(predict_title_text[i], test_title_text[i])\n",
    "        rouge_1_f = rouge_1_f + scores[0]['rouge-1']['f']\n",
    "        rouge_2_f = rouge_2_f + scores[0]['rouge-2']['f']\n",
    "        rouge_l_f = rouge_l_f + scores[0]['rouge-l']['f']\n",
    "        \n",
    "        if i%1000==0:\n",
    "            print(\"ROUGE-l-f:\", rouge_l_f)\n",
    "        \n",
    "    print(\"ROUGE-1:\", rouge_1_f / len(test_body_text))\n",
    "    print(\"ROUGE-2:\", rouge_2_f / len(test_body_text))\n",
    "    print(\"ROUGE-l:\", rouge_l_f / len(test_body_text))\n",
    "    print(\"Average of ROUGE-1, ROUGE-2 and ROUGE-l: \", (rouge_1_f + rouge_2_f + rouge_l_f) / 3 / len(test_body_text))\n",
    "\n",
    "calculate_rouge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "JTWAo-B8RAad"
   ],
   "name": "summarizer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
