{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Process-Data\" data-toc-modified-id=\"Process-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Process Data</a></span></li><li><span><a href=\"#Pre-Process-Data-For-Deep-Learning\" data-toc-modified-id=\"Pre-Process-Data-For-Deep-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pre-Process Data For Deep Learning</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Look-at-one-example-of-processed-issue-bodies\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-bodies-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Look at one example of processed issue bodies</a></span></li><li><span><a href=\"#Look-at-one-example-of-processed-issue-titles\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-titles-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Look at one example of processed issue titles</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define Model Architecture</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data-from-disk-into-variables\" data-toc-modified-id=\"Load-the-data-from-disk-into-variables-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Load the data from disk into variables</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Define Model Architecture</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#See-Results-On-Holdout-Set\" data-toc-modified-id=\"See-Results-On-Holdout-Set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>See Results On Holdout Set</a></span></li><li><span><a href=\"#Feature-Extraction-Demo\" data-toc-modified-id=\"Feature-Extraction-Demo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Extraction Demo</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1:-Issues-Installing-Python-Packages\" data-toc-modified-id=\"Example-1:-Issues-Installing-Python-Packages-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Example 1: Issues Installing Python Packages</a></span></li><li><span><a href=\"#Example-2:--Issues-asking-for-feature-improvements\" data-toc-modified-id=\"Example-2:--Issues-asking-for-feature-improvements-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Example 2:  Issues asking for feature improvements</a></span></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at filesystem to see files extracted from BigQuery (or Kaggle: https://www.kaggle.com/davidshinn/github-issues/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah | grep github_issues.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set and preview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1,800,000 rows 3 columns\n",
      "Test: 200,000 rows 3 columns\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271751</th>\n",
       "      <td>\"https://github.com/luciddreamz/laravel-ex/issues/4\"</td>\n",
       "      <td>how to have local git instead of github?</td>\n",
       "      <td>how do i update my project from my local pc instead of github? i were able to do that on openshift v2 but i can't understand how the v3 image work. can you provide a simple guide on this?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5108112</th>\n",
       "      <td>\"https://github.com/robert-ciobotaru/Proiect_IP/issues/4\"</td>\n",
       "      <td>un protocol de comunicare cu middle-end-ul.</td>\n",
       "      <td>avem nevoie de un protocol detaliat pentru comunicarea cu middle-end-ul.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3476565</th>\n",
       "      <td>\"https://github.com/PhenX/php-font-lib/issues/58\"</td>\n",
       "      <td>what are the compatible font extensions?</td>\n",
       "      <td>i need to validate a form for font submission, what kind of files should i accept?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         issue_url  \\\n",
       "271751        \"https://github.com/luciddreamz/laravel-ex/issues/4\"   \n",
       "5108112  \"https://github.com/robert-ciobotaru/Proiect_IP/issues/4\"   \n",
       "3476565          \"https://github.com/PhenX/php-font-lib/issues/58\"   \n",
       "\n",
       "                                         issue_title  \\\n",
       "271751      how to have local git instead of github?   \n",
       "5108112  un protocol de comunicare cu middle-end-ul.   \n",
       "3476565     what are the compatible font extensions?   \n",
       "\n",
       "                                                                                                                                                                                                body  \n",
       "271751   how do i update my project from my local pc instead of github? i were able to do that on openshift v2 but i can't understand how the v3 image work. can you provide a simple guide on this?  \n",
       "5108112                                                                                                                     avem nevoie de un protocol detaliat pentru comunicarea cu middle-end-ul.  \n",
       "3476565                                                                                                           i need to validate a form for font submission, what kind of files should i accept?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read in data sample 2M rows (for speed of tutorial)\n",
    "traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=2000000), \n",
    "                                   test_size=.10)\n",
    "\n",
    "\n",
    "#print out stats about shape of data\n",
    "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
    "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
    "\n",
    "# preview data\n",
    "traindf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert to lists in preparation for modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"how do i update my project from my local pc instead of github? i were able to do that on openshift v2 but i can't understand how the v3 image work. can you provide a simple guide on this?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_body_raw = traindf.body.tolist()\n",
    "train_title_raw = traindf.issue_title.tolist()\n",
    "#preview output of first element\n",
    "train_body_raw[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy==2.0.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/f6/4a61c2707f8006131abc4d4d3428b8a34e2e540d1ab115a2997ae2475d6d/spacy-2.0.15-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (25.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 25.6MB 2.3MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3,>=0.2 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (0.2.9)\n",
      "Collecting regex==2018.01.10 (from spacy==2.0.15)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/f4/7146c3812f96fcaaf2d06ff6862582302626a59011ccb6f2833bb38d80f7/regex-2018.01.10.tar.gz (612kB)\n",
      "\u001b[K    100% |████████████████████████████████| 614kB 9.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.13.0,>=6.12.0 (from spacy==2.0.15)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/84/a4d8e8b66729ec0f1bc676ed6614333fedefb5ac49235d065067192715e5/thinc-6.12.1-cp37-cp37m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (2.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.6MB 6.9MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting msgpack-numpy<0.4.4.0murmurhash>=0.28.0,<1.1.0 (from spacy==2.0.15)\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/4c/318e321a1f369be6bc4a1c3416d4e6472e7f9439b30d9bac70bf387019b0/msgpack_numpy-0.4.4-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (1.15.4)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (0.9.6)\n",
      "Collecting ujson>=1.35 (from spacy==2.0.15)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (2.21.0)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.7/site-packages (from spacy==2.0.15) (2.0.1)\n",
      "Requirement already satisfied: msgpack<0.6.0,>=0.5.6 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (0.5.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (4.28.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (1.10.11)\n",
      "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (0.9.0.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (1.0.2)\n",
      "Requirement already satisfied: six<2.0.0,>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy==2.0.15) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.15) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.15) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.15) (1.24.1)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.15) (3.0.4)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from cytoolz<0.10,>=0.9.0->thinc<6.13.0,>=6.12.0->spacy==2.0.15) (0.9.0)\n",
      "Building wheels for collected packages: regex\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/shirley/Library/Caches/pip/wheels/74/17/3f/c77bba99efd74ba1a19862c9dd97f4b6d735e2826721dc00ff\n",
      "Successfully built regex\n",
      "\u001b[31mthinc 6.12.1 has requirement msgpack-numpy<0.4.4, but you'll have msgpack-numpy 0.4.4 which is incompatible.\u001b[0m\n",
      "Installing collected packages: regex, msgpack-numpy, thinc, ujson, spacy\n",
      "  Found existing installation: msgpack-numpy 0.4.4.2\n",
      "    Uninstalling msgpack-numpy-0.4.4.2:\n",
      "      Successfully uninstalled msgpack-numpy-0.4.4.2\n",
      "  Found existing installation: thinc 7.0.4\n",
      "    Uninstalling thinc-7.0.4:\n",
      "      Successfully uninstalled thinc-7.0.4\n",
      "  Found existing installation: spacy 2.1.3\n",
      "    Uninstalling spacy-2.1.3:\n",
      "      Successfully uninstalled spacy-2.1.3\n",
      "Successfully installed msgpack-numpy-0.4.4 regex-2018.1.10 spacy-2.0.15 thinc-6.12.1 ujson-1.35\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ktext\n",
      "  Using cached https://files.pythonhosted.org/packages/da/18/577262a30a7cf39a0a5b11815d9aed8792afb4db8e1ac63e5d727da90f8f/ktext-0.34-py3-none-any.whl\n",
      "Requirement already satisfied: msgpack-numpy in /anaconda3/lib/python3.7/site-packages (from ktext) (0.4.4)\n",
      "Requirement already satisfied: msgpack in /anaconda3/lib/python3.7/site-packages (from ktext) (0.5.6)\n",
      "Requirement already satisfied: numpy in /anaconda3/lib/python3.7/site-packages (from ktext) (1.15.4)\n",
      "Requirement already satisfied: pyyaml in /anaconda3/lib/python3.7/site-packages (from ktext) (3.13)\n",
      "Requirement already satisfied: pathos in /anaconda3/lib/python3.7/site-packages (from ktext) (0.2.3)\n",
      "Requirement already satisfied: scipy in /anaconda3/lib/python3.7/site-packages (from ktext) (1.1.0)\n",
      "Requirement already satisfied: more-itertools in /anaconda3/lib/python3.7/site-packages (from ktext) (4.3.0)\n",
      "Requirement already satisfied: tensorflow in /anaconda3/lib/python3.7/site-packages (from ktext) (1.13.1)\n",
      "Requirement already satisfied: pandas>=0.21.0 in /anaconda3/lib/python3.7/site-packages (from ktext) (0.23.4)\n",
      "Requirement already satisfied: dask in /anaconda3/lib/python3.7/site-packages (from ktext) (1.0.0)\n",
      "Requirement already satisfied: keras in /anaconda3/lib/python3.7/site-packages (from ktext) (2.1.2)\n",
      "Collecting textacy<=0.6.2 (from ktext)\n",
      "  Using cached https://files.pythonhosted.org/packages/f7/13/77612f4393d9c8a55e53924f13b2cf8b835cbf4a5e69e288613ed2de9eca/textacy-0.6.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six in /anaconda3/lib/python3.7/site-packages (from ktext) (1.12.0)\n",
      "Requirement already satisfied: pyarrow in /anaconda3/lib/python3.7/site-packages (from ktext) (0.12.1)\n",
      "Requirement already satisfied: dill>=0.2.9 in /anaconda3/lib/python3.7/site-packages (from pathos->ktext) (0.2.9)\n",
      "Requirement already satisfied: pox>=0.2.5 in /anaconda3/lib/python3.7/site-packages (from pathos->ktext) (0.2.5)\n",
      "Requirement already satisfied: ppft>=1.6.4.9 in /anaconda3/lib/python3.7/site-packages (from pathos->ktext) (1.6.4.9)\n",
      "Requirement already satisfied: multiprocess>=0.70.7 in /anaconda3/lib/python3.7/site-packages (from pathos->ktext) (0.70.7)\n",
      "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.13.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (0.7.1)\n",
      "Requirement already satisfied: tensorboard<1.14.0,>=1.13.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.13.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.0.9)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.0.7)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (1.19.0)\n",
      "Requirement already satisfied: absl-py>=0.1.6 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (0.7.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (3.7.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /anaconda3/lib/python3.7/site-packages (from tensorflow->ktext) (0.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /anaconda3/lib/python3.7/site-packages (from pandas>=0.21.0->ktext) (2.7.5)\n",
      "Requirement already satisfied: pytz>=2011k in /anaconda3/lib/python3.7/site-packages (from pandas>=0.21.0->ktext) (2018.7)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.2.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (4.4.3)\n",
      "Requirement already satisfied: tqdm>=4.11.1 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (4.28.1)\n",
      "Requirement already satisfied: unidecode>=0.04.19 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (1.0.23)\n",
      "Requirement already satisfied: cytoolz>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (0.9.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.17.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (0.20.1)\n",
      "Requirement already satisfied: python-levenshtein>=0.12.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (0.12.0)\n",
      "Requirement already satisfied: ijson>=2.3 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (2.3)\n",
      "Requirement already satisfied: requests>=2.10.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (2.21.0)\n",
      "Requirement already satisfied: spacy>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (2.0.15)\n",
      "Collecting pyphen>=0.9.4 (from textacy<=0.6.2->ktext)\n",
      "  Using cached https://files.pythonhosted.org/packages/15/82/08a3629dce8d1f3d91db843bb36d4d7db6b6269d5067259613a0d5c8a9db/Pyphen-0.9.5-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (3.1.0)\n",
      "Requirement already satisfied: networkx>=1.11 in /anaconda3/lib/python3.7/site-packages (from textacy<=0.6.2->ktext) (2.2)\n",
      "Collecting pyemd>=0.3.0 (from textacy<=0.6.2->ktext)\n",
      "  Using cached https://files.pythonhosted.org/packages/c0/c5/7fea8e7a71cd026b30ed3c40e4c5ea13a173e28f8855da17e25271e8f545/pyemd-0.5.1.tar.gz\n",
      "Requirement already satisfied: mock>=2.0.0 in /anaconda3/lib/python3.7/site-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->ktext) (2.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->ktext) (3.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /anaconda3/lib/python3.7/site-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow->ktext) (0.14.1)\n",
      "Requirement already satisfied: h5py in /anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.6->tensorflow->ktext) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow->ktext) (40.6.3)\n",
      "Requirement already satisfied: wcwidth in /anaconda3/lib/python3.7/site-packages (from ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (0.1.7)\n",
      "Requirement already satisfied: html5lib in /anaconda3/lib/python3.7/site-packages (from ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (1.0.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /anaconda3/lib/python3.7/site-packages (from cytoolz>=0.8.0->textacy<=0.6.2->ktext) (0.9.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (2018.11.29)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.7/site-packages (from requests>=2.10.0->textacy<=0.6.2->ktext) (2.8)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (0.9.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2.0.2)\n",
      "Requirement already satisfied: regex==2018.01.10 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2018.1.10)\n",
      "Requirement already satisfied: thinc<6.13.0,>=6.12.0 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (6.12.1)\n",
      "Requirement already satisfied: ujson>=1.35 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (1.35)\n",
      "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /anaconda3/lib/python3.7/site-packages (from spacy>=2.0.0->textacy<=0.6.2->ktext) (2.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /anaconda3/lib/python3.7/site-packages (from networkx>=1.11->textacy<=0.6.2->ktext) (4.3.0)\n",
      "Requirement already satisfied: pbr>=0.11 in /anaconda3/lib/python3.7/site-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0->tensorflow->ktext) (5.1.3)\n",
      "Requirement already satisfied: webencodings in /anaconda3/lib/python3.7/site-packages (from html5lib->ftfy<5.0.0,>=4.2.0->textacy<=0.6.2->ktext) (0.5.1)\n",
      "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy>=2.0.0->textacy<=0.6.2->ktext) (1.10.11)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /anaconda3/lib/python3.7/site-packages (from thinc<6.13.0,>=6.12.0->spacy>=2.0.0->textacy<=0.6.2->ktext) (1.0.2)\n",
      "Building wheels for collected packages: pyemd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Running setup.py bdist_wheel for pyemd ... \u001b[?25lerror\n",
      "  Complete output from command /anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-install-4oaa1sn0/pyemd/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" bdist_wheel -d /private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-wheel-q48561iq --python-tag cp37:\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7\n",
      "  creating build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "  copying pyemd/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "  copying pyemd/__about__.py -> build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "  running build_ext\n",
      "  building 'pyemd.emd' extension\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7\n",
      "  creating build/temp.macosx-10.7-x86_64-3.7/pyemd\n",
      "  gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/anaconda3/include -arch x86_64 -I/anaconda3/include -arch x86_64 -Ipyemd -I/anaconda3/include/python3.7m -I/anaconda3/lib/python3.7/site-packages/numpy/core/include -c pyemd/emd.cpp -o build/temp.macosx-10.7-x86_64-3.7/pyemd/emd.o\n",
      "  warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "  pyemd/emd.cpp:644:10: fatal error: 'ios' file not found\n",
      "  #include \"ios\"\n",
      "           ^~~~~\n",
      "  1 warning and 1 error generated.\n",
      "  error: command 'gcc' failed with exit status 1\n",
      "  \n",
      "  ----------------------------------------\n",
      "\u001b[31m  Failed building wheel for pyemd\u001b[0m\n",
      "\u001b[?25h  Running setup.py clean for pyemd\n",
      "Failed to build pyemd\n",
      "Installing collected packages: pyphen, pyemd, textacy, ktext\n",
      "  Running setup.py install for pyemd ... \u001b[?25lerror\n",
      "    Complete output from command /anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-install-4oaa1sn0/pyemd/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-record-o4c7or6g/install-record.txt --single-version-externally-managed --compile:\n",
      "    running install\n",
      "    running build\n",
      "    running build_py\n",
      "    creating build\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7\n",
      "    creating build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "    copying pyemd/__init__.py -> build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "    copying pyemd/__about__.py -> build/lib.macosx-10.7-x86_64-3.7/pyemd\n",
      "    running build_ext\n",
      "    building 'pyemd.emd' extension\n",
      "    creating build/temp.macosx-10.7-x86_64-3.7\n",
      "    creating build/temp.macosx-10.7-x86_64-3.7/pyemd\n",
      "    gcc -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -I/anaconda3/include -arch x86_64 -I/anaconda3/include -arch x86_64 -Ipyemd -I/anaconda3/include/python3.7m -I/anaconda3/lib/python3.7/site-packages/numpy/core/include -c pyemd/emd.cpp -o build/temp.macosx-10.7-x86_64-3.7/pyemd/emd.o\n",
      "    warning: include path for stdlibc++ headers not found; pass '-std=libc++' on the command line to use the libc++ standard library instead [-Wstdlibcxx-not-found]\n",
      "    pyemd/emd.cpp:644:10: fatal error: 'ios' file not found\n",
      "    #include \"ios\"\n",
      "             ^~~~~\n",
      "    1 warning and 1 error generated.\n",
      "    error: command 'gcc' failed with exit status 1\n",
      "    \n",
      "    ----------------------------------------\n",
      "\u001b[31mCommand \"/anaconda3/bin/python -u -c \"import setuptools, tokenize;__file__='/private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-install-4oaa1sn0/pyemd/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\\r\\n', '\\n');f.close();exec(compile(code, __file__, 'exec'))\" install --record /private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-record-o4c7or6g/install-record.txt --single-version-externally-managed --compile\" failed with error code 1 in /private/var/folders/67/tx0qhqqj59b04kxb_6kkzjt00000gn/T/pip-install-4oaa1sn0/pyemd/\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Process Data For Deep Learning\n",
    "\n",
    "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from ktext.preprocess import processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 491 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 68 sec\n",
      "WARNING:root:Finished parsing 1,800,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 48 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 13s, sys: 38.5 s, total: 3min 52s\n",
      "Wall time: 10min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
    "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
    "#  to 1 which will become common index for rare words \n",
    "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
    "train_body_vecs = body_pp.fit_transform(train_body_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue bodies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " how do i update my project from my local pc instead of github? i were able to do that on openshift v2 but i can't understand how the v3 image work. can you provide a simple guide on this? \n",
      "\n",
      "after pre-processing:\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0   72   52    6  176   41  116   23   41  205 1538  168   11\n",
      "  212    6  421  207    4   52   17   16 2855 1030   24    6   27   30\n",
      "  791   72    3 1518  100   81   27   20  324    5  503 1067   16   13] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
    "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:....tokenizing data\n",
      "WARNING:root:(1/2) done. 63 sec\n",
      "WARNING:root:....building corpus\n",
      "WARNING:root:(2/2) done. 9 sec\n",
      "WARNING:root:Finished parsing 1,800,000 documents.\n",
      "WARNING:root:...fit is finished, beginning transform\n",
      "WARNING:root:...padding data\n",
      "WARNING:root:done. 12 sec\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a text processor for the titles, with some different parameters\n",
    "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
    "#                      document\n",
    "#  padding = 'post' means that zero padding is appended to the end of the \n",
    "#             of the document (as opposed to the default which is 'pre')\n",
    "title_pp = processor(append_indicators=True, keep_n=4500, \n",
    "                     padding_maxlen=12, padding ='post')\n",
    "\n",
    "# process the title data\n",
    "train_title_vecs = title_pp.fit_transform(train_title_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at one example of processed issue titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "original string:\n",
      " how to have local git instead of github?\n",
      "after pre-processing:\n",
      " [  2  26   4  97 184 382 142  12 166   3   0   0]\n"
     ]
    }
   ],
   "source": [
    "print('\\noriginal string:\\n', train_title_raw[0])\n",
    "print('after pre-processing:\\n', train_title_vecs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize all of this to disk for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill as dpickle\n",
    "import numpy as np\n",
    "\n",
    "# Save the preprocessor\n",
    "with open('body_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(body_pp, f)\n",
    "\n",
    "with open('title_pp.dpkl', 'wb') as f:\n",
    "    dpickle.dump(title_pp, f)\n",
    "\n",
    "# Save the processed data\n",
    "np.save('train_title_vecs.npy', train_title_vecs)\n",
    "np.save('train_body_vecs.npy', train_body_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data from disk into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting annoy\n",
      "Installing collected packages: annoy\n",
      "Successfully installed annoy-1.15.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install annoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of encoder input: (1800000, 70)\n",
      "Shape of decoder input: (1800000, 11)\n",
      "Shape of decoder target: (1800000, 11)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
    "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of vocabulary for body_pp.dpkl: 8,002\n",
      "Size of vocabulary for title_pp.dpkl: 4,502\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
    "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/test_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/envs/test_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#arbitrarly set latent dimension for embedding and hidden units\n",
    "latent_dim = 300\n",
    "\n",
    "##### Define Model Architecture ######\n",
    "\n",
    "########################\n",
    "#### Encoder Model ####\n",
    "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
    "\n",
    "# Word embeding for encoder (ex: Issue Body)\n",
    "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
    "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
    "\n",
    "# Intermediate GRU layer (optional)\n",
    "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
    "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
    "\n",
    "# We do not need the `encoder_output` just the hidden state.\n",
    "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
    "\n",
    "# Encapsulate the encoder as a separate entity so we can just \n",
    "#  encode without decoding if we want to.\n",
    "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
    "\n",
    "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
    "\n",
    "########################\n",
    "#### Decoder Model ####\n",
    "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
    "\n",
    "# Word Embedding For Decoder (ex: Issue Titles)\n",
    "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
    "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
    "\n",
    "# Set up the decoder, using `decoder_state_input` as initial state.\n",
    "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
    "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
    "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
    "\n",
    "# Dense layer for prediction\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "########################\n",
    "#### Seq2Seq Model ####\n",
    "\n",
    "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
    "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "\n",
    "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling pydot-1.4.1:\r\n",
      "  Would remove:\r\n",
      "    /anaconda3/envs/test_env/lib/python3.6/site-packages/dot_parser.py\r\n",
      "    /anaconda3/envs/test_env/lib/python3.6/site-packages/pydot-1.4.1.dist-info/*\r\n",
      "    /anaconda3/envs/test_env/lib/python3.6/site-packages/pydot.py\r\n",
      "Proceed (y/n)? "
     ]
    }
   ],
   "source": [
    "# !pip uninstall pydot\n",
    "# !pip install pydot\n",
    "# !pip install pydotplus\n",
    "# !pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Examine Model Architecture Summary **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Decoder-Input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Word-Embedding (Embeddi (None, None, 300)    1350600     Decoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Input (InputLayer)      (None, 70)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-1 (BatchNorma (None, None, 300)    1200        Decoder-Word-Embedding[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-Model (Model)           (None, 300)          2942700     Encoder-Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-GRU (GRU)               [(None, None, 300),  540900      Decoder-Batchnorm-1[0][0]        \n",
      "                                                                 Encoder-Model[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Decoder-Batchnorm-2 (BatchNorma (None, None, 300)    1200        Decoder-GRU[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Final-Output-Dense (Dense)      (None, None, 4502)   1355102     Decoder-Batchnorm-2[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 6,191,702\n",
      "Trainable params: 6,189,902\n",
      "Non-trainable params: 1,800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from seq2seq_utils import viz_model_architecture\n",
    "seq2seq_Model.summary()\n",
    "# viz_model_architecture(seq2seq_Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1584000 samples, validate on 216000 samples\n",
      "Epoch 1/1\n",
      "1584000/1584000 [==============================] - 8904s 6ms/step - loss: 2.6674 - val_loss: 2.4202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/test_env/lib/python3.6/site-packages/keras/engine/network.py:877: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_3:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
    "\n",
    "script_name_base = 'tutorial_seq2seq'\n",
    "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
    "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
    "                                   save_best_only=True)\n",
    "\n",
    "batch_size = 1200\n",
    "epochs = 1\n",
    "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ds/.local/lib/python3.6/site-packages/keras/engine/topology.py:2344: UserWarning: Layer Decoder-GRU was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'Encoder-Model/Encoder-Last-GRU/while/Exit_2:0' shape=(?, 300) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# See Results On Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'annoy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e3cc2638f1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mseq2seq_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSeq2Seq_Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n\u001b[1;32m      3\u001b[0m                                  \u001b[0mdecoder_preprocessor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtitle_pp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  seq2seq_model=seq2seq_Model)\n",
      "\u001b[0;32m~/Desktop/notebooks/On_old_dataset/seq2seq_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdill\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mannoy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnoyIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'annoy'"
     ]
    }
   ],
   "source": [
    "\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                 decoder_preprocessor=title_pp,\n",
    "                                 seq2seq_model=seq2seq_Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this method displays the predictions on random rows of the holdout set\n",
    "ref_title, gen_title = seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"ref_title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rouge in /anaconda3/lib/python3.7/site-packages (0.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read All 5M data points\n",
    "all_data_df = pd.read_csv('github_issues.csv')\n",
    "# Extract the bodies from this dataframe\n",
    "all_data_bodies = all_data_df['body'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform all of the data using the ktext processor\n",
    "all_data_vectorized = body_pp.transform_parallel(all_data_bodies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save transformed data\n",
    "with open('all_data_vectorized.dpkl', 'wb') as f:\n",
    "    dpickle.dump(all_data_vectorized, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from seq2seq_utils import Seq2Seq_Inference\n",
    "seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
    "                                    decoder_preprocessor=title_pp,\n",
    "                                    seq2seq_model=seq2seq_Model)\n",
    "recsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Example 1: Issues Installing Python Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 13563 =================\n",
      "\n",
      "\"https://github.com/bnosac/pattern.nlp/issues/5\"\n",
      "Issue Body:\n",
      " thanks for your package, i can't wait to use it. unfortunately i have issues with the installation. prerequisite is 'first install python version 2.5+ not version 3 '. so this package cant be used with version 3.6 64bit that i have installed? i nevertheless tried to install it using pip, conda is not supported? but got an error: 'syntaxerror: missing parentheses in call to 'print''. besides when i try to run the library in r version 3.3.3. 64 bit i got errors with can_find_python_cmd required_modules = pattern.db : 'error in find_python_cmd......' pattern seems to be written in python but must be used in r, why cant it be used in python? i found another python pattern application that apparently does the same in python: https://pypi.python.org/pypi/pattern how is this related? \n",
      "\n",
      "Original Title:\n",
      " error installation python\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " install with python * number *\n",
      "\n",
      "**** Similar Issues (using encoder embedding) ****:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>286906</th>\n",
       "      <td>\"https://github.com/scikit-hep/root_numpy/issues/337\"</td>\n",
       "      <td>root 6.10/02 and root_numpy compatibility</td>\n",
       "      <td>i am trying to pip install root_pandas and one of the dependency is root_numpy however some weird reasons i am unable to install it even though i can import root in python. i am working on python3.6 as i am more comfortable with it. is root_numpy is not yet compatible with the latest root?</td>\n",
       "      <td>0.694671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>314005</th>\n",
       "      <td>\"https://github.com/andim/noisyopt/issues/4\"</td>\n",
       "      <td>joss review: installing dependencies via pip</td>\n",
       "      <td>hi, i'm trying to install noisyopt in a clean conda environment running python 3.5. running pip install noisyopt does not install the dependencies numpy, scipy . i see that you do include a requires keyword argument in your setup.py file, does this need to be install_requires ? as in https://packaging.python.org/requirements/ . also, not necessary if you don't want to, but i think it would be good to include a list of dependences somewhere in the readme.</td>\n",
       "      <td>0.698265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48120</th>\n",
       "      <td>\"https://github.com/turi-code/SFrame/issues/389\"</td>\n",
       "      <td>python 3.6 compatible</td>\n",
       "      <td>hi: i tried to install sframe using pip and conda but i can not find anything that will work with python 3.6? has sframe been updated to work with python 3.6 yet? thanks, drew</td>\n",
       "      <td>0.718715</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    issue_url  \\\n",
       "286906  \"https://github.com/scikit-hep/root_numpy/issues/337\"   \n",
       "314005           \"https://github.com/andim/noisyopt/issues/4\"   \n",
       "48120        \"https://github.com/turi-code/SFrame/issues/389\"   \n",
       "\n",
       "                                         issue_title  \\\n",
       "286906     root 6.10/02 and root_numpy compatibility   \n",
       "314005  joss review: installing dependencies via pip   \n",
       "48120                          python 3.6 compatible   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                              body  \\\n",
       "286906                                                                                                                                                                          i am trying to pip install root_pandas and one of the dependency is root_numpy however some weird reasons i am unable to install it even though i can import root in python. i am working on python3.6 as i am more comfortable with it. is root_numpy is not yet compatible with the latest root?   \n",
       "314005  hi, i'm trying to install noisyopt in a clean conda environment running python 3.5. running pip install noisyopt does not install the dependencies numpy, scipy . i see that you do include a requires keyword argument in your setup.py file, does this need to be install_requires ? as in https://packaging.python.org/requirements/ . also, not necessary if you don't want to, but i think it would be good to include a list of dependences somewhere in the readme.   \n",
       "48120                                                                                                                                                                                                                                                                                              hi: i tried to install sframe using pip and conda but i can not find anything that will work with python 3.6? has sframe been updated to work with python 3.6 yet? thanks, drew   \n",
       "\n",
       "            dist  \n",
       "286906  0.694671  \n",
       "314005  0.698265  \n",
       "48120   0.718715  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2:  Issues asking for feature improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==============================================\n",
      "============== Example # 157322 =================\n",
      "\n",
      "\"https://github.com/Chingu-cohorts/devgaido/issues/89\"\n",
      "Issue Body:\n",
      " right now, your profile link is https://devgaido.com/profile. this is fine, but it would be really cool if there was a way to share your profile with other people. on my portfolio, i have social media buttons to freecodecamp, github, ect. without a custom link, i cannot show-off what i have done on devgaido to future employers. \n",
      "\n",
      "Original Title:\n",
      " feature request: sharable profile.\n",
      "\n",
      "****** Machine Generated Title (Prediction) ******:\n",
      " add a link to your profile\n",
      "\n",
      "**** Similar Issues (using encoder embedding) ****:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>issue_url</th>\n",
       "      <th>issue_title</th>\n",
       "      <th>body</th>\n",
       "      <th>dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>250423</th>\n",
       "      <td>\"https://github.com/ParabolInc/action/issues/1379\"</td>\n",
       "      <td>integrations list view discoverability</td>\n",
       "      <td>issue - enhancement i was initially confused by the link to my account copy; seeing github in the integrations list made me think it had already been set up . i realize now that i had to allow parabol to post as me. i think that link to my account could use a tooltip explaining what link means, and why you'd want to do so. &lt;img width= 728 alt= screen shot 2017-09-29 at 10 52 05 am src= https://user-images.githubusercontent.com/2146312/31024786-2fd39c46-a50e-11e7-9f2a-6d4a5ed2baeb.png &gt;</td>\n",
       "      <td>0.748828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222304</th>\n",
       "      <td>\"https://github.com/viosey/hexo-theme-material/issues/166\"</td>\n",
       "      <td>allow us to use sns-share for github</td>\n",
       "      <td>i'd love to be able to add a link at the bottom of the page for my github account. however, the sns-share option doesn't currently seem to be able to do this.</td>\n",
       "      <td>0.774398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153327</th>\n",
       "      <td>\"https://github.com/tobykurien/GoogleApps/issues/31\"</td>\n",
       "      <td>drive provide download ability</td>\n",
       "      <td>sometimes people share files via g drive. provided a link this app can show some info about the files but doesn't show the download button. i hope that it can be fixed and users would be able to download files with this app.</td>\n",
       "      <td>0.778953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         issue_url  \\\n",
       "250423          \"https://github.com/ParabolInc/action/issues/1379\"   \n",
       "222304  \"https://github.com/viosey/hexo-theme-material/issues/166\"   \n",
       "153327        \"https://github.com/tobykurien/GoogleApps/issues/31\"   \n",
       "\n",
       "                                   issue_title  \\\n",
       "250423  integrations list view discoverability   \n",
       "222304    allow us to use sns-share for github   \n",
       "153327          drive provide download ability   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              body  \\\n",
       "250423  issue - enhancement i was initially confused by the link to my account copy; seeing github in the integrations list made me think it had already been set up . i realize now that i had to allow parabol to post as me. i think that link to my account could use a tooltip explaining what link means, and why you'd want to do so. <img width= 728 alt= screen shot 2017-09-29 at 10 52 05 am src= https://user-images.githubusercontent.com/2146312/31024786-2fd39c46-a50e-11e7-9f2a-6d4a5ed2baeb.png >   \n",
       "222304                                                                                                                                                                                                                                                                                                                                              i'd love to be able to add a link at the bottom of the page for my github account. however, the sns-share option doesn't currently seem to be able to do this.   \n",
       "153327                                                                                                                                                                                                                                                                            sometimes people share files via g drive. provided a link this app can show some info about the files but doesn't show the download button. i hope that it can be fixed and users would be able to download files with this app.   \n",
       "\n",
       "            dist  \n",
       "250423  0.748828  \n",
       "222304  0.774398  \n",
       "153327  0.778953  "
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# incase you need to reset the rec system\n",
    "# seq2seq_inf_rec.set_recsys_annoyobj(recsys_annoyobj)\n",
    "# seq2seq_inf_rec.set_recsys_data(all_data_df)\n",
    "\n",
    "# save object\n",
    "recsys_annoyobj.save('recsys_annoyobj.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "nav_menu": {
    "height": "263px",
    "width": "352px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
