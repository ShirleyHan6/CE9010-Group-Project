{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarizer.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "toc": true,
        "id": "bNO2qvQH7kXX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Process-Data\" data-toc-modified-id=\"Process-Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Process Data</a></span></li><li><span><a href=\"#Pre-Process-Data-For-Deep-Learning\" data-toc-modified-id=\"Pre-Process-Data-For-Deep-Learning-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Pre-Process Data For Deep Learning</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Look-at-one-example-of-processed-issue-bodies\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-bodies-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>Look at one example of processed issue bodies</a></span></li><li><span><a href=\"#Look-at-one-example-of-processed-issue-titles\" data-toc-modified-id=\"Look-at-one-example-of-processed-issue-titles-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Look at one example of processed issue titles</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Define Model Architecture</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Load-the-data-from-disk-into-variables\" data-toc-modified-id=\"Load-the-data-from-disk-into-variables-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Load the data from disk into variables</a></span></li><li><span><a href=\"#Define-Model-Architecture\" data-toc-modified-id=\"Define-Model-Architecture-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Define Model Architecture</a></span></li></ul></li></ul></li><li><span><a href=\"#Train-Model\" data-toc-modified-id=\"Train-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Train Model</a></span></li><li><span><a href=\"#See-Results-On-Holdout-Set\" data-toc-modified-id=\"See-Results-On-Holdout-Set-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>See Results On Holdout Set</a></span></li><li><span><a href=\"#Feature-Extraction-Demo\" data-toc-modified-id=\"Feature-Extraction-Demo-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Feature Extraction Demo</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Example-1:-Issues-Installing-Python-Packages\" data-toc-modified-id=\"Example-1:-Issues-Installing-Python-Packages-6.0.1\"><span class=\"toc-item-num\">6.0.1&nbsp;&nbsp;</span>Example 1: Issues Installing Python Packages</a></span></li><li><span><a href=\"#Example-2:--Issues-asking-for-feature-improvements\" data-toc-modified-id=\"Example-2:--Issues-asking-for-feature-improvements-6.0.2\"><span class=\"toc-item-num\">6.0.2&nbsp;&nbsp;</span>Example 2:  Issues asking for feature improvements</a></span></li></ul></li></ul></li></ul></div>"
      ]
    },
    {
      "metadata": {
        "id": "LXj_sF0_797u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "743cd525-88c9-489c-e204-97ce7fcd9e27"
      },
      "cell_type": "code",
      "source": [
        "!pip install annoy"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting annoy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9c/bf/8e3f7051d694afc086184d223e892d0fc18aca1e4147042d0521a6adedb5/annoy-1.15.1.tar.gz (643kB)\n",
            "\u001b[K    100% |████████████████████████████████| 645kB 24.0MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: annoy\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/77/cb/7a/6f3ed44099e394e0cb0b6b41213b61fe6595b726530744f2ce\n",
            "Successfully built annoy\n",
            "Installing collected packages: annoy\n",
            "Successfully installed annoy-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZMwyFR5y7mfq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from IPython.display import SVG, display\n",
        "from keras.utils.vis_utils import model_to_dot\n",
        "import logging\n",
        "import numpy as np\n",
        "import dill as dpickle\n",
        "from annoy import AnnoyIndex\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from random import random\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "def load_text_processor(fname='title_pp.dpkl'):\n",
        "    \"\"\"\n",
        "    Load preprocessors from disk.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    fname: str\n",
        "        file name of ktext.proccessor object\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    num_tokens : int\n",
        "        size of vocabulary loaded into ktext.processor\n",
        "    pp : ktext.processor\n",
        "        the processor you are trying to load\n",
        "\n",
        "    Typical Usage:\n",
        "    -------------\n",
        "\n",
        "    num_decoder_tokens, title_pp = load_text_processor(fname='title_pp.dpkl')\n",
        "    num_encoder_tokens, body_pp = load_text_processor(fname='body_pp.dpkl')\n",
        "\n",
        "    \"\"\"\n",
        "    # Load files from disk\n",
        "    with open(fname, 'rb') as f:\n",
        "        pp = dpickle.load(f)\n",
        "\n",
        "    num_tokens = max(pp.id2token.keys()) + 1\n",
        "    print(f'Size of vocabulary for {fname}: {num_tokens:,}')\n",
        "    return num_tokens, pp\n",
        "\n",
        "\n",
        "def load_decoder_inputs(decoder_np_vecs='train_title_vecs.npy'):\n",
        "    \"\"\"\n",
        "    Load decoder inputs.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    decoder_np_vecs : str\n",
        "        filename of serialized numpy.array of decoder input (issue title)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    decoder_input_data : numpy.array\n",
        "        The data fed to the decoder as input during training for teacher forcing.\n",
        "        This is the same as `decoder_np_vecs` except the last position.\n",
        "    decoder_target_data : numpy.array\n",
        "        The data that the decoder data is trained to generate (issue title).\n",
        "        Calculated by sliding `decoder_np_vecs` one position forward.\n",
        "\n",
        "    \"\"\"\n",
        "    vectorized_title = np.load(decoder_np_vecs)\n",
        "    # For Decoder Input, you don't need the last word as that is only for prediction\n",
        "    # when we are training using Teacher Forcing.\n",
        "    decoder_input_data = vectorized_title[:, :-1]\n",
        "\n",
        "    # Decoder Target Data Is Ahead By 1 Time Step From Decoder Input Data (Teacher Forcing)\n",
        "    decoder_target_data = vectorized_title[:, 1:]\n",
        "\n",
        "    print(f'Shape of decoder input: {decoder_input_data.shape}')\n",
        "    print(f'Shape of decoder target: {decoder_target_data.shape}')\n",
        "    return decoder_input_data, decoder_target_data\n",
        "\n",
        "\n",
        "def load_encoder_inputs(encoder_np_vecs='train_body_vecs.npy'):\n",
        "    \"\"\"\n",
        "    Load variables & data that are inputs to encoder.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    encoder_np_vecs : str\n",
        "        filename of serialized numpy.array of encoder input (issue title)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    encoder_input_data : numpy.array\n",
        "        The issue body\n",
        "    doc_length : int\n",
        "        The standard document length of the input for the encoder after padding\n",
        "        the shape of this array will be (num_examples, doc_length)\n",
        "\n",
        "    \"\"\"\n",
        "    vectorized_body = np.load(encoder_np_vecs)\n",
        "    # Encoder input is simply the body of the issue text\n",
        "    encoder_input_data = vectorized_body\n",
        "    doc_length = encoder_input_data.shape[1]\n",
        "    print(f'Shape of encoder input: {encoder_input_data.shape}')\n",
        "    return encoder_input_data, doc_length\n",
        "\n",
        "\n",
        "def viz_model_architecture(model):\n",
        "    \"\"\"Visualize model architecture in Jupyter notebook.\"\"\"\n",
        "    display(SVG(model_to_dot(model).create(prog='dot', format='svg')))\n",
        "\n",
        "\n",
        "def free_gpu_mem():\n",
        "    \"\"\"Attempt to free gpu memory.\"\"\"\n",
        "    K.get_session().close()\n",
        "    cfg = K.tf.ConfigProto()\n",
        "    cfg.gpu_options.allow_growth = True\n",
        "    K.set_session(K.tf.Session(config=cfg))\n",
        "\n",
        "\n",
        "def test_gpu():\n",
        "    \"\"\"Run a toy computation task in tensorflow to test GPU.\"\"\"\n",
        "    config = tf.ConfigProto()\n",
        "    config.gpu_options.allow_growth = True\n",
        "    session = tf.Session(config=config)\n",
        "    hello = tf.constant('Hello, TensorFlow!')\n",
        "    print(session.run(hello))\n",
        "\n",
        "\n",
        "def plot_model_training_history(history_object):\n",
        "    \"\"\"Plots model train vs. validation loss.\"\"\"\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.plot(history_object.history['loss'])\n",
        "    plt.plot(history_object.history['val_loss'])\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def extract_encoder_model(model):\n",
        "    \"\"\"\n",
        "    Extract the encoder from the original Sequence to Sequence Model.\n",
        "\n",
        "    Returns a keras model object that has one input (body of issue) and one\n",
        "    output (encoding of issue, which is the last hidden state).\n",
        "\n",
        "    Input:\n",
        "    -----\n",
        "    model: keras model object\n",
        "\n",
        "    Returns:\n",
        "    -----\n",
        "    keras model object\n",
        "\n",
        "    \"\"\"\n",
        "    encoder_model = model.get_layer('Encoder-Model')\n",
        "    return encoder_model\n",
        "\n",
        "\n",
        "def extract_decoder_model(model):\n",
        "    \"\"\"\n",
        "    Extract the decoder from the original model.\n",
        "\n",
        "    Inputs:\n",
        "    ------\n",
        "    model: keras model object\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "    A Keras model object with the following inputs and outputs:\n",
        "\n",
        "    Inputs of Keras Model That Is Returned:\n",
        "    1: the embedding index for the last predicted word or the <Start> indicator\n",
        "    2: the last hidden state, or in the case of the first word the hidden state from the encoder\n",
        "\n",
        "    Outputs of Keras Model That Is Returned:\n",
        "    1.  Prediction (class probabilities) for the next word\n",
        "    2.  The hidden state of the decoder, to be fed back into the decoder at the next time step\n",
        "\n",
        "    Implementation Notes:\n",
        "    ----------------------\n",
        "    Must extract relevant layers and reconstruct part of the computation graph\n",
        "    to allow for different inputs as we are not going to use teacher forcing at\n",
        "    inference time.\n",
        "\n",
        "    \"\"\"\n",
        "    # the latent dimension is the same throughout the architecture so we are going to\n",
        "    # cheat and grab the latent dimension of the embedding because that is the same as what is\n",
        "    # output from the decoder\n",
        "    latent_dim = model.get_layer('Decoder-Word-Embedding').output_shape[-1]\n",
        "\n",
        "    # Reconstruct the input into the decoder\n",
        "    decoder_inputs = model.get_layer('Decoder-Input').input\n",
        "    dec_emb = model.get_layer('Decoder-Word-Embedding')(decoder_inputs)\n",
        "    dec_bn = model.get_layer('Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "    # Instead of setting the intial state from the encoder and forgetting about it, during inference\n",
        "    # we are not doing teacher forcing, so we will have to have a feedback loop from predictions back into\n",
        "    # the GRU, thus we define this input layer for the state so we can add this capability\n",
        "    gru_inference_state_input = Input(shape=(latent_dim,), name='hidden_state_input')\n",
        "\n",
        "    # we need to reuse the weights that is why we are getting this\n",
        "    # If you inspect the decoder GRU that we created for training, it will take as input\n",
        "    # 2 tensors -> (1) is the embedding layer output for the teacher forcing\n",
        "    #                  (which will now be the last step's prediction, and will be _start_ on the first time step)\n",
        "    #              (2) is the state, which we will initialize with the encoder on the first time step, but then\n",
        "    #                   grab the state after the first prediction and feed that back in again.\n",
        "    gru_out, gru_state_out = model.get_layer('Decoder-GRU')([dec_bn, gru_inference_state_input])\n",
        "\n",
        "    # Reconstruct dense layers\n",
        "    dec_bn2 = model.get_layer('Decoder-Batchnorm-2')(gru_out)\n",
        "    dense_out = model.get_layer('Final-Output-Dense')(dec_bn2)\n",
        "    decoder_model = Model([decoder_inputs, gru_inference_state_input],\n",
        "                          [dense_out, gru_state_out])\n",
        "    return decoder_model\n",
        "\n",
        "\n",
        "class Seq2Seq_Inference(object):\n",
        "    def __init__(self,\n",
        "                 encoder_preprocessor,\n",
        "                 decoder_preprocessor,\n",
        "                 seq2seq_model):\n",
        "\n",
        "        self.pp_body = encoder_preprocessor\n",
        "        self.pp_title = decoder_preprocessor\n",
        "        self.seq2seq_model = seq2seq_model\n",
        "        self.encoder_model = extract_encoder_model(seq2seq_model)\n",
        "        self.decoder_model = extract_decoder_model(seq2seq_model)\n",
        "        self.default_max_len_title = self.pp_title.padding_maxlen\n",
        "        self.nn = None\n",
        "        self.rec_df = None\n",
        "\n",
        "    def generate_issue_title(self,\n",
        "                             raw_input_text,\n",
        "                             max_len_title=None):\n",
        "        \"\"\"\n",
        "        Use the seq2seq model to generate a title given the body of an issue.\n",
        "\n",
        "        Inputs\n",
        "        ------\n",
        "        raw_input: str\n",
        "            The body of the issue text as an input string\n",
        "\n",
        "        max_len_title: int (optional)\n",
        "            The maximum length of the title the model will generate\n",
        "\n",
        "        \"\"\"\n",
        "        if max_len_title is None:\n",
        "            max_len_title = self.default_max_len_title\n",
        "        # get the encoder's features for the decoder\n",
        "        raw_tokenized = self.pp_body.transform([raw_input_text])\n",
        "        body_encoding = self.encoder_model.predict(raw_tokenized)\n",
        "        # we want to save the encoder's embedding before its updated by decoder\n",
        "        #   because we can use that as an embedding for other tasks.\n",
        "        original_body_encoding = body_encoding\n",
        "        state_value = np.array(self.pp_title.token2id['_start_']).reshape(1, 1)\n",
        "\n",
        "        decoded_sentence = []\n",
        "        stop_condition = False\n",
        "        while not stop_condition:\n",
        "            preds, st = self.decoder_model.predict([state_value, body_encoding])\n",
        "\n",
        "            # We are going to ignore indices 0 (padding) and indices 1 (unknown)\n",
        "            # Argmax will return the integer index corresponding to the\n",
        "            #  prediction + 2 b/c we chopped off first two\n",
        "            pred_idx = np.argmax(preds[:, :, 2:]) + 2\n",
        "\n",
        "            # retrieve word from index prediction\n",
        "            pred_word_str = self.pp_title.id2token[pred_idx]\n",
        "\n",
        "            if pred_word_str == '_end_' or len(decoded_sentence) >= max_len_title:\n",
        "                stop_condition = True\n",
        "                break\n",
        "            decoded_sentence.append(pred_word_str)\n",
        "\n",
        "            # update the decoder for the next word\n",
        "            body_encoding = st\n",
        "            state_value = np.array(pred_idx).reshape(1, 1)\n",
        "\n",
        "        return original_body_encoding, ' '.join(decoded_sentence)\n",
        "\n",
        "    def print_example(self,\n",
        "                      i,\n",
        "                      body_text,\n",
        "                      title_text,\n",
        "                      url,\n",
        "                      threshold):\n",
        "        \"\"\"\n",
        "        Prints an example of the model's prediction for manual inspection.\n",
        "        \"\"\"\n",
        "        # if i:\n",
        "        #     print('\\n\\n==============================================')\n",
        "        #     print(f'============== Example # {i} =================\\n')\n",
        "        #\n",
        "        # if url:\n",
        "        #     print(url)\n",
        "        #\n",
        "        # print(f\"Issue Body:\\n {body_text} \\n\")\n",
        "        #\n",
        "        # if title_text:\n",
        "        #     print(f\"Original Title:\\n {title_text}\")\n",
        "\n",
        "        emb, gen_title = self.generate_issue_title(body_text)\n",
        "        # print(f\"\\n****** Machine Generated Title (Prediction) ******:\\n {gen_title}\")\n",
        "\n",
        "        if self.nn:\n",
        "            # return neighbors and distances\n",
        "            n, d = self.nn.get_nns_by_vector(emb.flatten(), n=4,\n",
        "                                             include_distances=True)\n",
        "            neighbors = n[1:]\n",
        "            dist = d[1:]\n",
        "\n",
        "            if min(dist) <= threshold:\n",
        "                cols = ['issue_url', 'issue_title', 'body']\n",
        "                dfcopy = self.rec_df.iloc[neighbors][cols].copy(deep=True)\n",
        "                dfcopy['dist'] = dist\n",
        "                similar_issues_df = dfcopy.query(f'dist <= {threshold}')\n",
        "\n",
        "                print(\"\\n**** Similar Issues (using encoder embedding) ****:\\n\")\n",
        "                display(similar_issues_df)\n",
        "\n",
        "        return gen_title\n",
        "\n",
        "    def demo_model_predictions(self,\n",
        "                               n,\n",
        "                               issue_df,\n",
        "                               threshold=1):\n",
        "        \"\"\"\n",
        "        Pick n random Issues and display predictions.\n",
        "\n",
        "        Input:\n",
        "        ------\n",
        "        n : int\n",
        "            Number of issues to display from issue_df\n",
        "        issue_df : pandas DataFrame\n",
        "            DataFrame that contains two columns: `body` and `issue_title`.\n",
        "        threshold : float\n",
        "            distance threshold for recommendation of similar issues.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        None\n",
        "            Prints the original issue body and the model's prediction.\n",
        "        \"\"\"\n",
        "        # Extract body and title from DF\n",
        "        body_text = issue_df.body.tolist()\n",
        "        title_text = issue_df.issue_title.tolist()\n",
        "        url = issue_df.issue_url.tolist()\n",
        "\n",
        "        demo_list = np.random.randint(low=1, high=len(body_text), size=n)\n",
        "        gen_title = []\n",
        "        for i in demo_list:\n",
        "             gen_title.append(self.print_example(i,\n",
        "                               body_text=body_text[i],\n",
        "                               title_text=title_text[i],\n",
        "                               url=url[i],\n",
        "                               threshold=threshold))\n",
        "\n",
        "        return body_text, title_text, gen_title\n",
        "\n",
        "\n",
        "    def prepare_recommender(self, vectorized_array, original_df):\n",
        "        \"\"\"\n",
        "        Use the annoy library to build recommender\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        vectorized_array : List[List[int]]\n",
        "            This is the list of list of integers that represents your corpus\n",
        "            that is fed into the seq2seq model for training.\n",
        "        original_df : pandas.DataFrame\n",
        "            This is the original dataframe that has the columns\n",
        "            ['issue_url', 'issue_title', 'body']\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        annoy.AnnoyIndex  object (see https://github.com/spotify/annoy)\n",
        "        \"\"\"\n",
        "        self.rec_df = original_df\n",
        "        emb = self.encoder_model.predict(x=vectorized_array,\n",
        "                                         batch_size=vectorized_array.shape[0]//200)\n",
        "\n",
        "        f = emb.shape[1]\n",
        "        self.nn = AnnoyIndex(f)\n",
        "        logging.warning('Adding embeddings')\n",
        "        for i in tqdm(range(len(emb))):\n",
        "            self.nn.add_item(i, emb[i])\n",
        "        logging.warning('Building trees for similarity lookup.')\n",
        "        self.nn.build(50)\n",
        "        return self.nn\n",
        "\n",
        "    def set_recsys_data(self, original_df):\n",
        "        self.rec_df = original_df\n",
        "\n",
        "    def set_recsys_annoyobj(self, annoyobj):\n",
        "        self.nn = annoyobj\n",
        "\n",
        "    def evaluate_model(self, holdout_bodies, holdout_titles):\n",
        "        \"\"\"\n",
        "        Method for calculating BLEU Score.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        holdout_bodies : List[str]\n",
        "            These are the issue bodies that we want to summarize\n",
        "        holdout_titles : List[str]\n",
        "            This is the ground truth we are trying to predict --> issue titles\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        bleu : float\n",
        "            The BLEU Score\n",
        "\n",
        "        \"\"\"\n",
        "        actual, predicted = list(), list()\n",
        "        assert len(holdout_bodies) == len(holdout_titles)\n",
        "        num_examples = len(holdout_bodies)\n",
        "\n",
        "        logging.warning('Generating predictions.')\n",
        "        # step over the whole set TODO: parallelize this\n",
        "        for i in tqdm_notebook(range(num_examples)):\n",
        "            _, yhat = self.generate_issue_title(holdout_bodies[i])\n",
        "\n",
        "            actual.append(self.pp_title.process_text([holdout_titles[i]])[0])\n",
        "            predicted.append(self.pp_title.process_text([yhat])[0])\n",
        "        # calculate BLEU score\n",
        "        logging.warning('Calculating BLEU.')\n",
        "        \n",
        "        #must be careful with nltk api for corpus_bleu!, \n",
        "        # expects List[List[List[str]]] for ground truth, using List[List[str]] will give you\n",
        "        # erroneous results.\n",
        "        bleu = corpus_bleu([[a] for a in actual], predicted)\n",
        "        return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M4igbkkc7kXZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import logging\n",
        "import glob\n",
        "from sklearn.model_selection import train_test_split\n",
        "pd.set_option('display.max_colwidth', 500)\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.WARNING)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VWkkyvPc7kXc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Process Data"
      ]
    },
    {
      "metadata": {
        "id": "kVwxAJcm7kXd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Look at filesystem to see files extracted from BigQuery (or Kaggle: https://www.kaggle.com/davidshinn/github-issues/)"
      ]
    },
    {
      "metadata": {
        "id": "fMSonncF7kXe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!ls -lah | grep github_issues.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nRbYxTE-BlKP",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "outputId": "f5874584-6870-49f3-dfc5-19210b893880"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "upload = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b1ed480d-caab-4031-a740-8cf3312c2f7c\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-b1ed480d-caab-4031-a740-8cf3312c2f7c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DsNPpJkgCH2q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for fn in upload.keys():\n",
        "  print('User upload file \"{name}\" with (length) {length} bytes'.format(name = fn, length = len(upload[fn])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GJ-aBCQN7kXg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Split data into train and test set and preview data"
      ]
    },
    {
      "metadata": {
        "id": "38npN-zJ7kXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "outputId": "8ceef947-b281-4018-e207-dd94ea6c2248"
      },
      "cell_type": "code",
      "source": [
        "#read in data sample 2M rows (for speed of tutorial)\n",
        "traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=2000000), \n",
        "                                   test_size=.10)\n",
        "\n",
        "\n",
        "#print out stats about shape of data\n",
        "print(f'Train: {traindf.shape[0]:,} rows {traindf.shape[1]:,} columns')\n",
        "print(f'Test: {testdf.shape[0]:,} rows {testdf.shape[1]:,} columns')\n",
        "\n",
        "# preview data\n",
        "traindf.head(3)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-492ee524628d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m traindf, testdf = train_test_split(pd.read_csv('github_issues.csv').sample(n=2000000), \n\u001b[0m\u001b[1;32m      2\u001b[0m                                    test_size=.10)\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print out stats about shape of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 818\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1047\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1050\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1693\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: File b'github_issues.csv' does not exist"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eOJ6odRX7kXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Convert to lists in preparation for modeling**"
      ]
    },
    {
      "metadata": {
        "id": "tZTXhpL87kXk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_body_raw = traindf.body.tolist()\n",
        "train_title_raw = traindf.issue_title.tolist()\n",
        "#preview output of first element\n",
        "train_body_raw[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oqrWiJzG7kXm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cMFj8GYx7kXo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zywQw_Zi7kXq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-Process Data For Deep Learning\n",
        "\n",
        "See [this repo](https://github.com/hamelsmu/ktext) for documentation on the ktext package"
      ]
    },
    {
      "metadata": {
        "id": "LonbY3oy7kXr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from ktext.preprocess import processor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SAPyivf27kXv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Clean, tokenize, and apply padding / truncating such that each document length = 70\n",
        "#  also, retain only the top 8,000 words in the vocabulary and set the remaining words\n",
        "#  to 1 which will become common index for rare words \n",
        "body_pp = processor(keep_n=8000, padding_maxlen=70)\n",
        "train_body_vecs = body_pp.fit_transform(train_body_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SzvBgqg37kXx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Look at one example of processed issue bodies"
      ]
    },
    {
      "metadata": {
        "id": "lsUuF8Ds7kXy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('\\noriginal string:\\n', train_body_raw[0], '\\n')\n",
        "print('after pre-processing:\\n', train_body_vecs[0], '\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1bIiKdXj7kX0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Instantiate a text processor for the titles, with some different parameters\n",
        "#  append_indicators = True appends the tokens '_start_' and '_end_' to each\n",
        "#                      document\n",
        "#  padding = 'post' means that zero padding is appended to the end of the \n",
        "#             of the document (as opposed to the default which is 'pre')\n",
        "title_pp = processor(append_indicators=True, keep_n=4500, \n",
        "                     padding_maxlen=12, padding ='post')\n",
        "\n",
        "# process the title data\n",
        "train_title_vecs = title_pp.fit_transform(train_title_raw)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "I5wv818O7kX2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Look at one example of processed issue titles"
      ]
    },
    {
      "metadata": {
        "id": "st9sTRVA7kX3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('\\noriginal string:\\n', train_title_raw[0])\n",
        "print('after pre-processing:\\n', train_title_vecs[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cPUqBBBH7kX5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Serialize all of this to disk for later use"
      ]
    },
    {
      "metadata": {
        "id": "6gzcPFf47kX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import dill as dpickle\n",
        "import numpy as np\n",
        "\n",
        "# Save the preprocessor\n",
        "with open('body_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(body_pp, f)\n",
        "\n",
        "with open('title_pp.dpkl', 'wb') as f:\n",
        "    dpickle.dump(title_pp, f)\n",
        "\n",
        "# Save the processed data\n",
        "np.save('train_title_vecs.npy', train_title_vecs)\n",
        "np.save('train_body_vecs.npy', train_body_vecs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GP4jC4Kb7kX8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "w6rLPyCH7kX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load the data from disk into variables"
      ]
    },
    {
      "metadata": {
        "id": "ARMt4Rec7kX-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from seq2seq_utils import load_decoder_inputs, load_encoder_inputs, load_text_processor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xNnF_BHV7kYB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip install annoy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dh4WUmcq7kYE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoder_input_data, doc_length = load_encoder_inputs('train_body_vecs.npy')\n",
        "decoder_input_data, decoder_target_data = load_decoder_inputs('train_title_vecs.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYYMm0HB7kYJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_encoder_tokens, body_pp = load_text_processor('body_pp.dpkl')\n",
        "num_decoder_tokens, title_pp = load_text_processor('title_pp.dpkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cjLO81QP7kYL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define Model Architecture"
      ]
    },
    {
      "metadata": {
        "id": "4HPFtPvh7kYM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, GRU, Dense, Embedding, Bidirectional, BatchNormalization\n",
        "from keras import optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kIuu5Vq47kYR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#arbitrarly set latent dimension for embedding and hidden units\n",
        "latent_dim = 300\n",
        "\n",
        "##### Define Model Architecture ######\n",
        "\n",
        "########################\n",
        "#### Encoder Model ####\n",
        "encoder_inputs = Input(shape=(doc_length,), name='Encoder-Input')\n",
        "\n",
        "# Word embeding for encoder (ex: Issue Body)\n",
        "x = Embedding(num_encoder_tokens, latent_dim, name='Body-Word-Embedding', mask_zero=False)(encoder_inputs)\n",
        "x = BatchNormalization(name='Encoder-Batchnorm-1')(x)\n",
        "\n",
        "# Intermediate GRU layer (optional)\n",
        "#x = GRU(latent_dim, name='Encoder-Intermediate-GRU', return_sequences=True)(x)\n",
        "#x = BatchNormalization(name='Encoder-Batchnorm-2')(x)\n",
        "\n",
        "# We do not need the `encoder_output` just the hidden state.\n",
        "_, state_h = GRU(latent_dim, return_state=True, name='Encoder-Last-GRU')(x)\n",
        "\n",
        "# Encapsulate the encoder as a separate entity so we can just \n",
        "#  encode without decoding if we want to.\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=state_h, name='Encoder-Model')\n",
        "\n",
        "seq2seq_encoder_out = encoder_model(encoder_inputs)\n",
        "\n",
        "########################\n",
        "#### Decoder Model ####\n",
        "decoder_inputs = Input(shape=(None,), name='Decoder-Input')  # for teacher forcing\n",
        "\n",
        "# Word Embedding For Decoder (ex: Issue Titles)\n",
        "dec_emb = Embedding(num_decoder_tokens, latent_dim, name='Decoder-Word-Embedding', mask_zero=False)(decoder_inputs)\n",
        "dec_bn = BatchNormalization(name='Decoder-Batchnorm-1')(dec_emb)\n",
        "\n",
        "# Set up the decoder, using `decoder_state_input` as initial state.\n",
        "decoder_gru = GRU(latent_dim, return_state=True, return_sequences=True, name='Decoder-GRU')\n",
        "decoder_gru_output, _ = decoder_gru(dec_bn, initial_state=seq2seq_encoder_out)\n",
        "x = BatchNormalization(name='Decoder-Batchnorm-2')(decoder_gru_output)\n",
        "\n",
        "# Dense layer for prediction\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='Final-Output-Dense')\n",
        "decoder_outputs = decoder_dense(x)\n",
        "\n",
        "########################\n",
        "#### Seq2Seq Model ####\n",
        "\n",
        "#seq2seq_decoder_out = decoder_model([decoder_inputs, seq2seq_encoder_out])\n",
        "seq2seq_Model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "\n",
        "seq2seq_Model.compile(optimizer=optimizers.Nadam(lr=0.001), loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQSfY0lA7kYW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !pip uninstall pydot\n",
        "# !pip install pydot\n",
        "# !pip install pydotplus\n",
        "# !pip install graphviz"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FrFVMUml7kYZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Examine Model Architecture Summary **"
      ]
    },
    {
      "metadata": {
        "id": "emtDrpAB7kYa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from seq2seq_utils import viz_model_architecture\n",
        "seq2seq_Model.summary()\n",
        "# viz_model_architecture(seq2seq_Model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BPSD9yq67kYc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Train Model"
      ]
    },
    {
      "metadata": {
        "id": "MIcfNUDX7kYd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.callbacks import CSVLogger, ModelCheckpoint\n",
        "\n",
        "script_name_base = 'tutorial_seq2seq'\n",
        "csv_logger = CSVLogger('{:}.log'.format(script_name_base))\n",
        "model_checkpoint = ModelCheckpoint('{:}.epoch{{epoch:02d}}-val{{val_loss:.5f}}.hdf5'.format(script_name_base),\n",
        "                                   save_best_only=True)\n",
        "\n",
        "batch_size = 1200\n",
        "epochs = 1\n",
        "history = seq2seq_Model.fit([encoder_input_data, decoder_input_data], np.expand_dims(decoder_target_data, -1),\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_split=0.12, callbacks=[csv_logger, model_checkpoint])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UYkV3dW67kYf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#save model\n",
        "seq2seq_Model.save('seq2seq_model_tutorial.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s8WuvLXL7kYh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# See Results On Holdout Set"
      ]
    },
    {
      "metadata": {
        "id": "xmh5MjAA7kYi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "seq2seq_inf = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
        "                                 decoder_preprocessor=title_pp,\n",
        "                                 seq2seq_model=seq2seq_Model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": false,
        "id": "5yOyAZ-A7kYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# this method displays the predictions on random rows of the holdout set\n",
        "seq2seq_inf.demo_model_predictions(n=50, issue_df=testdf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vSOdMX6z7kYm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Feature Extraction Demo"
      ]
    },
    {
      "metadata": {
        "id": "MmhodfG37kYn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read All 5M data points\n",
        "all_data_df = pd.read_csv('github_issues.csv')\n",
        "# Extract the bodies from this dataframe\n",
        "all_data_bodies = all_data_df['body'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qxkFxWPw7kYp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# transform all of the data using the ktext processor\n",
        "all_data_vectorized = body_pp.transform_parallel(all_data_bodies)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DCN9n7Px7kYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# save transformed data\n",
        "with open('all_data_vectorized.dpkl', 'wb') as f:\n",
        "    dpickle.dump(all_data_vectorized, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6rsPao727kY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from seq2seq_utils import Seq2Seq_Inference\n",
        "seq2seq_inf_rec = Seq2Seq_Inference(encoder_preprocessor=body_pp,\n",
        "                                    decoder_preprocessor=title_pp,\n",
        "                                    seq2seq_model=seq2seq_Model)\n",
        "recsys_annoyobj = seq2seq_inf_rec.prepare_recommender(all_data_vectorized, all_data_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "id": "HWoin2JN7kY3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 1: Issues Installing Python Packages"
      ]
    },
    {
      "metadata": {
        "id": "FHB7TVjm7kY3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZP3LPp0S7kY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Example 2:  Issues asking for feature improvements"
      ]
    },
    {
      "metadata": {
        "id": "2HmC42197kY5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "seq2seq_inf_rec.demo_model_predictions(n=1, issue_df=testdf, threshold=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMlX2T0z7kY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# incase you need to reset the rec system\n",
        "# seq2seq_inf_rec.set_recsys_annoyobj(recsys_annoyobj)\n",
        "# seq2seq_inf_rec.set_recsys_data(all_data_df)\n",
        "\n",
        "# save object\n",
        "recsys_annoyobj.save('recsys_annoyobj.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UT0CuGlA7kY8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}